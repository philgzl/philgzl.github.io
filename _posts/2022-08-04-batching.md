---
layout: post
title: "Batching of variable size inputs with PyTorch"
date: 2022-08-04 00:00:00 +0000
categories:
    - deep learning
    - batching
    - PyTorch
---

Batching training examples when optimizing a neural network usually relies on training examples being the same size. However, when dealing with sequential data, which is usually the case in speech processing, training examples are usually not same length. Iterating the training routine one observation at the time is usually not viable, so some techniques are required to form batches of variable size inputs and improve training timings.

# Padding and collating

One common solution is to simply zero-pad all training examples to match the longest example. This can be done as a one-time preprocessing step and does not require any extra implementation. However, if the length of training examples is highly variable, this is highly inefficient as the total size of the dataset can substantially increase. Moreover, the neural network can become biased towards zeros, unless precautions are taken when calculating the loss (see further below).

A much better solution is to pad the observations on the batch level. Observations are zero-padded to match the longest example within the batch, instead of the whole dataset. As a result, the amount of zeros padded is substantially smaller. In PyTorch, this can be done by defining a custom collating function and using the `collate_fn` argument when initializing the DataLoader, or by subclassing the DataLoader class as shown below. In this example, the neural network input and output are sequences of the same length, so they both need to be padded.

```python
import torch
import torch.nn.functional as F


class PaddingDataLoader(torch.utils.data.DataLoader):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.collate_fn = self._collate_fn

    def _collate_fn(self, batch):
        lengths = list(x.shape[-1] for x, _ in batch)
        max_length = max(lengths)
        batch_x, batch_y = [], []
        for x, y in batch:
            # x and y have shape (..., length)
            padding = max_length - x.shape[-1]
            batch_x.append(F.pad(x, (0, padding)))
            batch_y.append(F.pad(y, (0, padding)))
        return torch.stack(batch_x), torch.stack(batch_y), lengths
```

Note I return the original length of each observation in the batch together with the padded inputs and labels. This is because further down the training routine, the loss must be calculated over the original sequence length, such that the model doesn't become biased towards zeros. This can be done by applying a "mask" to the neural network output and target before calculating the sample-wise loss, such that the points outside the mask have no contribution after reducing the loss. Below are two examples of custom losses using such mask: a signal-to-noise ratio (SNR) loss and a mean squared error (MSE) loss.

```python
eps = torch.finfo(torch.float32).eps


class SNR:
    def __call__(self, output, target, lengths):
        # output and target have shape (batch_size, ..., length)
        output, target = apply_mask(output, target, lengths)
        snr = target.pow(2).sum(-1)/((target-output).pow(2).sum(-1) + eps)  # (batch_size, ...)
        snr = 10*torch.log10(snr + eps)  # (batch_size, ...)
        return -torch.mean(snr)


class MSE:
    def __call__(self, output, target, lengths):
        # output and target have shape (batch_size, ..., length)
        output, target = apply_mask(output, target, lengths)
        loss = (output-target).pow(2).sum(-1)  # (batch_size, ...)
        for i in range(output.shape[0]):
            loss[i] /= lengths[i]
        return loss.mean()


def apply_mask(output, target, lengths):
    mask = torch.zeros(*output.shape, device=output.device)
    for i, length in enumerate(lengths):
        mask[i, ..., :length:] = 1
    return output*mask, target*mask
```

# Batching strategies

While padding on a batch level might be enough for most use cases, training timings can be further reduced by using different batching strategies. Namely, the amount of padding can be minimized by batching observations of similar length. Sorting all observations by length before batching allows minimum padding, but sacrifices randomness as the same observations are batched together from one training iteration to another. Conversely, batching completely random observations together results in a high amount of padding. It becomes clear that there is a compromise between padding reduction and randomness in the presentation of training examples to the neural network.

One batching strategy with a tuneable parameter that sets the compromise between padding and randomness is called bucket batching. Bucket batching consists in grouping observations of similar length into "buckets", and forming batches using observations from the same bucket. The number of buckets sets the compromise between randomness and padding; using a single bucket results in random batching, while using as many buckets as observations results in sorted batching. In TensorFlow, bucket batching is implemented under [tf.data.Dataset.bucket_by_sequence_length](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#bucket_by_sequence_length), but there is currently no equivalent in PyTorch (actually after writing this I realized [TorchData](https://github.com/pytorch/data) recently released [BucketBatcher](https://pytorch.org/data/main/generated/torchdata.datapipes.iter.BucketBatcher.html), but TorchData "does not have a stable release" as of writing this).

Training with variable size batches can come with caveats. First, gradients can become less consistent, since they are calculated over a variable number of points, which can lead to stability issues. Second, the instantaneous amount of used GPU memory can become highly variable, which means the total amount of allocated GPU memory is not used to its full extent. These two caveats can be overcome by using a dynamic batch size; the batch size is not defined in terms of a number of training examples, but as a total duration. In other words, the batch capacity (batch size &times; batch length) is kept constant. Fig. 1 illustrates the random, sorted and bucket batching strategies, using either fixed or dynamic batch sizes.

<figure style="text-align: center;">
  <img src="/assets/img/batching.png"> 
  <figcaption><b>Fig. 1:</b> Illustration of different batching strategies. Adjacent observations of the same color are batched together. Padding is represented in black. For bucket batching, 3 buckets are used.</figcaption>
</figure>

In PyTorch, custom batch samplers can be implemented and assigned to a DataLoader through the `batch_sampler` argument. In the following I will show how to implement three batching strategies: random batching, sorted batching and bucket batching. All three batch sampler will derive from the same custom base class and support both fixed and dynamic batch sizes.

## Base batch sampler class

Extra features of this base batch sampler compared to the default batch sampler from PyTorch include:
- A `dynamic` attribute that determines if the batch size is fixed or not.
- A `sort` attribute that determines if observations should be sorted before creating the batches. The random and sorted batching strategies will thus naturally result from the base class by setting this attribute.
- A `set_epoch` method that sets the random seed according to the current epoch. This is absolutely necessary to make reproducible experiments when training is interrupted and needs to be resumed.

Note that the Dataset instance assigned to the batch sampler needs an `item_lengths` attribute with the list of all training example lengths. While this can be calculated by iterating over the dataset once, in my case I already calculate and store it when initializing my Dataset instance (not shown here).

The `__pre_init__` method is used as a routine that needs to be called before anything else for all subclasses, even those overriding the `__init__` method (there is probably a cleaner way of doing that). This will show useful when implementing the bucket batch sampler.

```python
import random


class BaseBatchSampler(torch.utils.data.Sampler):
    def __init__(self, dataset, batch_size, drop_last=False, shuffle=True,
                 seed=0, dynamic=False, sort=False):
        self.__pre_init__(dataset, batch_size, drop_last, shuffle, seed,
                          dynamic, sort)
        self.generate_batches()

    def __pre_init__(self, dataset, batch_size, drop_last, shuffle, seed,
                     dynamic, sort):
        self.dataset = dataset
        self.batch_size = batch_size
        self.drop_last = drop_last
        self.shuffle = shuffle
        self.seed = seed
        self.dynamic = dynamic
        self.sort = sort
        self._epoch = 0
        self._previous_epoch = -1
        self._item_lengths = self.get_item_lengths()

    def generate_batches(self):
        indices = self._generate_indices()
        self._generate_batches(indices)

    def _generate_indices(self):
        if self.sort:
            if self.shuffle:
                # sort by length but randomize items of same length
                randomizer = random.Random(self.seed + self._epoch)
                lengths = sorted(self._item_lengths,
                                 key=lambda x: (x[1], randomizer.random()))
            else:
                lengths = sorted(self._item_lengths, key=lambda x: x[1])
            indices = [idx for idx, length in lengths]
        else:
            indices = list(range(len(self._item_lengths)))
            if self.shuffle:
                randomizer = random.Random(self.seed + self._epoch)
                randomizer.shuffle(indices)
        return indices

    def _new_batch(self, batch, item_length):
        output = False
        if self.dynamic:
            if item_length > self.batch_size:
                raise ValueError('found an item that is longer than the '
                                 'dynamic batch size')
            batch_length = max(item[1] for item in batch) if batch else 0
            if (len(batch)+1)*max(item_length, batch_length) > self.batch_size:
                output = True
        elif len(batch)+1 > self.batch_size:
            output = True
        return output

    def _generate_batches(self, indices):
        self.batches = []
        batch = []
        for i in indices:
            item_idx, item_length = self._item_lengths[i]
            if self._new_batch(batch, item_length):
                self.batches.append(batch)
                batch = []
                batch.append((item_idx, item_length))
            else:
                batch.append((item_idx, item_length))
        if len(batch) > 0 and not self.drop_last:
            self.batches.append(batch)

    def shuffle_batches(self):
        randomizer = random.Random(self.seed + self._epoch)
        randomizer.shuffle(self.batches)

    def get_item_lengths(self):
        if isinstance(self.dataset, torch.utils.data.Subset):
            dataset = self.dataset.dataset
            indices = self.dataset.indices
            lengths = []
            for i, index in enumerate(indices):
                lengths.append((i, dataset.item_lengths[index]))
        else:
            lengths = list(enumerate(self.dataset.item_lengths))
        return lengths

    def set_epoch(self, epoch):
        self._epoch = epoch

    def __iter__(self):
        if self.shuffle:
            if self._epoch == self._previous_epoch:
                raise ValueError('the set_epoch method must be called before '
                                 'iterating over the dataloader in order to '
                                 'regenerate the batches with the correct '
                                 'seed')
            self.generate_batches()
            self.shuffle_batches()
            self._previous_epoch = self._epoch
        for batch in self.batches:
            yield [idx for idx, length in batch]

    def __len__(self):
        return len(self.batches)
```

## Random and sorted batching

Random and sorted batching naturally result from the base class by forcing the `sort` attribute.

```python
class RandomBatchSampler(BaseBatchSampler):
    def __init__(self, dataset, batch_size, drop_last=False, shuffle=True,
                 seed=0, dynamic=False):
        super().__init__(dataset, batch_size, drop_last=drop_last,
                         shuffle=shuffle, seed=seed, dynamic=dynamic,
                         sort=False)


class SortedBatchSampler(BaseBatchSampler):
    def __init__(self, dataset, batch_size, drop_last=False, shuffle=True,
                 seed=0, dynamic=False):
        super().__init__(dataset, batch_size, drop_last=drop_last,
                         shuffle=shuffle, seed=seed, dynamic=dynamic,
                         sort=True)
```

## Bucket batching

For the bucket batch sampler we need to rewrite the `__init__` method to add the `num_buckets` argument and set some new attributes before calling the `generate_batches` method (this is where `__pre_init__` comes handy as we still want to set most of the base attributes).

The bucket "limits" are uniformly distributed between 0 and the maximum training example length. Another option is to set the limits to quantiles of the distribution of lengths, such that each bucket contains the same number of observations.

```python
class BucketBatchSampler(BaseBatchSampler):
    def __init__(self, dataset, batch_size, num_buckets=10, drop_last=False,
                 shuffle=True, seed=0, dynamic=False):
        super().__pre_init__(dataset, batch_size, drop_last, shuffle, seed,
                             dynamic, sort=False)
        max_length = max(item[1] for item in self._item_lengths)
        self.max_length = max_length
        self.num_buckets = num_buckets
        self.right_bucket_limits = np.linspace(
            max_length/num_buckets, max_length, num_buckets,
        )
        if self.dynamic:
            self.bucket_batch_lengths = batch_size//self.right_bucket_limits
        else:
            self.bucket_batch_lengths = [batch_size]*self.num_buckets
        self.generate_batches()

    def _generate_batches(self, indices):
        batches = [[] for _ in range(self.num_buckets)]
        current_batches = [[] for _ in range(self.num_buckets)]
        for i in indices:
            item_idx, item_length = self._item_lengths[i]
            bucket_idx = np.searchsorted(
                self.right_bucket_limits, item_length,
            )
            if bucket_idx == self.num_buckets:
                if item_length == self.max_length:
                    bucket_idx -= 1
                else:
                    raise ValueError('found an item that is longer than the '
                                     'maximum item length')
            current_batches[bucket_idx].append((item_idx, item_length))
            if len(current_batches[bucket_idx]) \
                    == self.bucket_batch_lengths[bucket_idx]:
                batches[bucket_idx].append(current_batches[bucket_idx])
                current_batches[bucket_idx] = []
            elif len(current_batches[bucket_idx]) \
                    > self.bucket_batch_lengths[bucket_idx]:
                raise ValueError('maximum number of items in bucket exceeded')
        if not self.drop_last:
            for bucket_idx, batch in enumerate(current_batches):
                if len(batch) > 0:
                    batches[bucket_idx].append(batch)
        self.batches = [item for batch in batches for item in batch]
```
