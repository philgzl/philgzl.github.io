<!DOCTYPE html>
<html lang="en">

<head><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link href="https://fonts.googleapis.com/css?family=Merriweather:300|Raleway:400,700" rel="stylesheet">
<link rel="stylesheet" href="/assets/css/style.css">
<title>Batching of variable size inputs with PyTorch</title>
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Batching of variable size inputs with PyTorch | Philippe Gonzalez</title>
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="Batching of variable size inputs with PyTorch" />
<meta name="author" content="Philippe Gonzalez" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Batching training examples when optimizing a neural network usually relies on training examples being the same size. However, when dealing with sequential data, which is usually the case in speech processing, training examples are usually not same length. Iterating the training routine one observation at the time is usually not viable, so some techniques are required to form batches of variable size inputs and improve training timings." />
<meta property="og:description" content="Batching training examples when optimizing a neural network usually relies on training examples being the same size. However, when dealing with sequential data, which is usually the case in speech processing, training examples are usually not same length. Iterating the training routine one observation at the time is usually not viable, so some techniques are required to form batches of variable size inputs and improve training timings." />
<link rel="canonical" href="http://localhost:4000/posts/2022-08-04-batching" />
<meta property="og:url" content="http://localhost:4000/posts/2022-08-04-batching" />
<meta property="og:site_name" content="Philippe Gonzalez" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-08-04T02:00:00+02:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Batching of variable size inputs with PyTorch" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Philippe Gonzalez"},"dateModified":"2022-08-04T02:00:00+02:00","datePublished":"2022-08-04T02:00:00+02:00","description":"Batching training examples when optimizing a neural network usually relies on training examples being the same size. However, when dealing with sequential data, which is usually the case in speech processing, training examples are usually not same length. Iterating the training routine one observation at the time is usually not viable, so some techniques are required to form batches of variable size inputs and improve training timings.","headline":"Batching of variable size inputs with PyTorch","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/2022-08-04-batching"},"url":"http://localhost:4000/posts/2022-08-04-batching"}</script>
<!-- End Jekyll SEO tag -->


<script type="text/javascript" src="/assets/js/darkmode.js"></script>





</head><body>
  <main class="container">
    <section class="about">
      <div class="about-header condensed">
      <div class="about-title">
      <a href="/">
        
        <img src="/assets/img/me.png" alt="Philippe Gonzalez" />
        
      </a>
      <h2 id="title">
        <a href="/">Philippe Gonzalez</a>
      </h2>
      </div><p class="tagline">PhD student in speech enhancement</p></div>
      
      <ul class="social about-footer condensed"><a href="https://github.com/philgzl" target="_blank">
          <li>
            <i class="icon-github-circled"></i>
          </li>
        </a><a href="https://www.linkedin.com/in/philgzl" target="_blank">
          <li>
            <i class="icon-linkedin-squared"></i>
          </li>
        </a><a href="mailto:hello@philgzl.com" target="_blank">
          <li>
            <i class="icon-mail-alt"></i>
          </li>
        </a></ul><nav class="navigation about-footer condensed">
        <ul>
          
          <li>
            <a href="/about">About</a>
          </li>
          
        </ul>
      </nav><p class="about-footer condensed">&copy;
        2023</p><div class="about-footer condensed">
        <p>Dark Mode
          <i class="icon-moon"></i>
          <label class="switch">
            <input type="checkbox" class="dark-mode-toggle">
            <span class="slider round" onclick="toggleDarkMode()"></span>
          </label>
        </p>
      </div>
    </section>
    <section class="content">
      <div class="post-container">
  <a class="post-link" href="/posts/2022-08-04-batching">
    <h2 class="post-title">Batching of variable size inputs with PyTorch</h2>
  </a>
  <div class="post-meta">
    <div class="post-date"><i class="icon-calendar"></i>Aug 4, 2022</div><ul class="post-categories"><li>deep learning</li><li>batching</li><li>PyTorch</li></ul></div>
  <div class="post">
    <p>Batching training examples when optimizing a neural network usually relies on training examples being the same size. However, when dealing with sequential data, which is usually the case in speech processing, training examples are usually not same length. Iterating the training routine one observation at the time is usually not viable, so some techniques are required to form batches of variable size inputs and improve training timings.</p>

<h1 id="padding-and-collating">Padding and collating</h1>

<p>One common solution is to simply zero-pad all training examples to match the longest example. This can be done as a one-time preprocessing step and does not require any extra implementation. However, if the length of training examples is highly variable, this is highly inefficient as the total size of the dataset can substantially increase. Moreover, the neural network can become biased towards zeros, unless precautions are taken when calculating the loss (see further below).</p>

<p>A much better solution is to pad the observations on the batch level. Observations are zero-padded to match the longest example within the batch, instead of the whole dataset. As a result, the amount of zeros padded is substantially smaller. In PyTorch, this can be done by defining a custom collating function and using the <code class="language-plaintext highlighter-rouge">collate_fn</code> argument when initializing the DataLoader, or by subclassing the DataLoader class as shown below. In this example, the neural network input and output are sequences of the same length, so they both need to be padded.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>


<span class="k">class</span> <span class="nc">PaddingDataLoader</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">collate_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_collate_fn</span>

    <span class="k">def</span> <span class="nf">_collate_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span>
        <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
            <span class="c1"># x and y have shape (..., length)
</span>            <span class="n">padding</span> <span class="o">=</span> <span class="n">max_length</span> <span class="o">-</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">batch_x</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">padding</span><span class="p">)))</span>
            <span class="n">batch_y</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="n">pad</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">padding</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">batch_x</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">batch_y</span><span class="p">),</span> <span class="n">lengths</span>
</code></pre></div></div>

<p>Note I return the original length of each observation in the batch together with the padded inputs and labels. This is because further down the training routine, the loss must be calculated over the original sequence length, such that the model doesn’t become biased towards zeros. This can be done by applying a “mask” to the neural network output and target before calculating the sample-wise loss, such that the points outside the mask have no contribution after reducing the loss. Below are two examples of custom losses using such mask: a signal-to-noise ratio (SNR) loss and a mean squared error (MSE) loss.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="n">eps</span>


<span class="k">class</span> <span class="nc">SNR</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">lengths</span><span class="p">):</span>
        <span class="c1"># output and target have shape (batch_size, ..., length)
</span>        <span class="n">output</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">apply_mask</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
        <span class="n">snr</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">((</span><span class="n">target</span><span class="o">-</span><span class="n">output</span><span class="p">).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>  <span class="c1"># (batch_size, ...)
</span>        <span class="n">snr</span> <span class="o">=</span> <span class="mi">10</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">snr</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>  <span class="c1"># (batch_size, ...)
</span>        <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">snr</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MSE</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">lengths</span><span class="p">):</span>
        <span class="c1"># output and target have shape (batch_size, ..., length)
</span>        <span class="n">output</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">apply_mask</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">output</span><span class="o">-</span><span class="n">target</span><span class="p">).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, ...)
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">loss</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/=</span> <span class="n">lengths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">apply_mask</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">lengths</span><span class="p">):</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="o">*</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">output</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">length</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lengths</span><span class="p">):</span>
        <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">...,</span> <span class="p">:</span><span class="n">length</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">output</span><span class="o">*</span><span class="n">mask</span><span class="p">,</span> <span class="n">target</span><span class="o">*</span><span class="n">mask</span>
</code></pre></div></div>

<h1 id="batching-strategies">Batching strategies</h1>

<p>While padding on a batch level might be enough for most use cases, training timings can be further reduced by using different batching strategies. Namely, the amount of padding can be minimized by batching observations of similar length. Sorting all observations by length before batching allows minimum padding, but sacrifices randomness as the same observations are batched together from one training iteration to another. Conversely, batching completely random observations together results in a high amount of padding. It becomes clear that there is a compromise between padding reduction and randomness in the presentation of training examples to the neural network.</p>

<p>One batching strategy with a tuneable parameter that sets the compromise between padding and randomness is called bucket batching. Bucket batching consists in grouping observations of similar length into “buckets”, and forming batches using observations from the same bucket. The number of buckets sets the compromise between randomness and padding; using a single bucket results in random batching, while using as many buckets as observations results in sorted batching. In TensorFlow, bucket batching is implemented under <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#bucket_by_sequence_length">tf.data.Dataset.bucket_by_sequence_length</a>, but there is currently no equivalent in PyTorch (actually after writing this I realized <a href="https://github.com/pytorch/data">TorchData</a> recently released <a href="https://pytorch.org/data/main/generated/torchdata.datapipes.iter.BucketBatcher.html">BucketBatcher</a>, but TorchData “does not have a stable release” as of writing this).</p>

<p>Training with variable size batches can come with caveats. First, gradients can become less consistent, since they are calculated over a variable number of points, which can lead to stability issues. Second, the instantaneous amount of used GPU memory can become highly variable, which means the total amount of allocated GPU memory is not used to its full extent. These two caveats can be overcome by using a dynamic batch size; the batch size is not defined in terms of a number of training examples, but as a total duration. In other words, the batch capacity (batch size × batch length) is kept constant. Fig. 1 illustrates the random, sorted and bucket batching strategies, using either fixed or dynamic batch sizes.</p>

<figure style="text-align: center;">
  <img src="/assets/img/batching.png" /> 
  <figcaption><b>Fig. 1:</b> Illustration of different batching strategies. Adjacent observations of the same color are batched together. Padding is represented in black. For bucket batching, 3 buckets are used.</figcaption>
</figure>

<p>In PyTorch, custom batch samplers can be implemented and assigned to a DataLoader through the <code class="language-plaintext highlighter-rouge">batch_sampler</code> argument. In the following I will show how to implement three batching strategies: random batching, sorted batching and bucket batching. All three batch sampler will derive from the same custom base class and support both fixed and dynamic batch sizes.</p>

<h2 id="base-batch-sampler-class">Base batch sampler class</h2>

<p>Extra features of this base batch sampler compared to the default batch sampler from PyTorch include:</p>
<ul>
  <li>A <code class="language-plaintext highlighter-rouge">dynamic</code> attribute that determines if the batch size is fixed or not.</li>
  <li>A <code class="language-plaintext highlighter-rouge">sort</code> attribute that determines if observations should be sorted before creating the batches. The random and sorted batching strategies will thus naturally result from the base class by setting this attribute.</li>
  <li>A <code class="language-plaintext highlighter-rouge">set_epoch</code> method that sets the random seed according to the current epoch. This is absolutely necessary to make reproducible experiments when training is interrupted and needs to be resumed.</li>
</ul>

<p>Note that the Dataset instance assigned to the batch sampler needs an <code class="language-plaintext highlighter-rouge">item_lengths</code> attribute with the list of all training example lengths. While this can be calculated by iterating over the dataset once, in my case I already calculate and store it when initializing my Dataset instance (not shown here).</p>

<p>The <code class="language-plaintext highlighter-rouge">__pre_init__</code> method is used as a routine that needs to be called before anything else for all subclasses, even those overriding the <code class="language-plaintext highlighter-rouge">__init__</code> method (there is probably a cleaner way of doing that). This will show useful when implementing the bucket batch sampler.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">random</span>


<span class="k">class</span> <span class="nc">BaseBatchSampler</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Sampler</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                 <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">sort</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">__pre_init__</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_last</span><span class="p">,</span> <span class="n">shuffle</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span>
                          <span class="n">dynamic</span><span class="p">,</span> <span class="n">sort</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">generate_batches</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__pre_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_last</span><span class="p">,</span> <span class="n">shuffle</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span>
                     <span class="n">dynamic</span><span class="p">,</span> <span class="n">sort</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">drop_last</span> <span class="o">=</span> <span class="n">drop_last</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">shuffle</span> <span class="o">=</span> <span class="n">shuffle</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dynamic</span> <span class="o">=</span> <span class="n">dynamic</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sort</span> <span class="o">=</span> <span class="n">sort</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_epoch</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_previous_epoch</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_item_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_item_lengths</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">generate_batches</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_generate_indices</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_generate_batches</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_generate_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">sort</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">shuffle</span><span class="p">:</span>
                <span class="c1"># sort by length but randomize items of same length
</span>                <span class="n">randomizer</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">Random</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">seed</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">_epoch</span><span class="p">)</span>
                <span class="n">lengths</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_item_lengths</span><span class="p">,</span>
                                 <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">randomizer</span><span class="p">.</span><span class="n">random</span><span class="p">()))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">lengths</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_item_lengths</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">length</span> <span class="ow">in</span> <span class="n">lengths</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_item_lengths</span><span class="p">)))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">shuffle</span><span class="p">:</span>
                <span class="n">randomizer</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">Random</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">seed</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">_epoch</span><span class="p">)</span>
                <span class="n">randomizer</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">indices</span>

    <span class="k">def</span> <span class="nf">_new_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">item_length</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">dynamic</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">item_length</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">batch_size</span><span class="p">:</span>
                <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">'found an item that is longer than the '</span>
                                 <span class="s">'dynamic batch size'</span><span class="p">)</span>
            <span class="n">batch_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span> <span class="k">if</span> <span class="n">batch</span> <span class="k">else</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="nb">max</span><span class="p">(</span><span class="n">item_length</span><span class="p">,</span> <span class="n">batch_length</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">batch_size</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">batch_size</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">_generate_batches</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">batches</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">:</span>
            <span class="n">item_idx</span><span class="p">,</span> <span class="n">item_length</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_item_lengths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">_new_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">item_length</span><span class="p">):</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">batches</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">batch</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">item_idx</span><span class="p">,</span> <span class="n">item_length</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">batch</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">item_idx</span><span class="p">,</span> <span class="n">item_length</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">.</span><span class="n">drop_last</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">batches</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">shuffle_batches</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">randomizer</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">Random</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">seed</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">_epoch</span><span class="p">)</span>
        <span class="n">randomizer</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">batches</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_item_lengths</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Subset</span><span class="p">):</span>
            <span class="n">dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">dataset</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">indices</span>
            <span class="n">lengths</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">indices</span><span class="p">):</span>
                <span class="n">lengths</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">dataset</span><span class="p">.</span><span class="n">item_lengths</span><span class="p">[</span><span class="n">index</span><span class="p">]))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lengths</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">item_lengths</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">lengths</span>

    <span class="k">def</span> <span class="nf">set_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_epoch</span> <span class="o">=</span> <span class="n">epoch</span>

    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">shuffle</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">_epoch</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">_previous_epoch</span><span class="p">:</span>
                <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">'the set_epoch method must be called before '</span>
                                 <span class="s">'iterating over the dataloader in order to '</span>
                                 <span class="s">'regenerate the batches with the correct '</span>
                                 <span class="s">'seed'</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">generate_batches</span><span class="p">()</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">shuffle_batches</span><span class="p">()</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">_previous_epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_epoch</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">batches</span><span class="p">:</span>
            <span class="k">yield</span> <span class="p">[</span><span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">length</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">batches</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="random-and-sorted-batching">Random and sorted batching</h2>

<p>Random and sorted batching naturally result from the base class by forcing the <code class="language-plaintext highlighter-rouge">sort</code> attribute.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RandomBatchSampler</span><span class="p">(</span><span class="n">BaseBatchSampler</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                 <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="n">drop_last</span><span class="p">,</span>
                         <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="n">dynamic</span><span class="p">,</span>
                         <span class="n">sort</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SortedBatchSampler</span><span class="p">(</span><span class="n">BaseBatchSampler</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                 <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="n">drop_last</span><span class="p">,</span>
                         <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="n">dynamic</span><span class="p">,</span>
                         <span class="n">sort</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="bucket-batching">Bucket batching</h2>

<p>For the bucket batch sampler we need to rewrite the <code class="language-plaintext highlighter-rouge">__init__</code> method to add the <code class="language-plaintext highlighter-rouge">num_buckets</code> argument and set some new attributes before calling the <code class="language-plaintext highlighter-rouge">generate_batches</code> method (this is where <code class="language-plaintext highlighter-rouge">__pre_init__</code> comes handy as we still want to set most of the base attributes).</p>

<p>The bucket “limits” (<code class="language-plaintext highlighter-rouge">right_bucket_limits</code> attribute) are uniformly distributed between 0 and the maximum training example length. Another option is to set the limits to quantiles of the distribution of lengths, such that each bucket contains the same number of observations.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BucketBatchSampler</span><span class="p">(</span><span class="n">BaseBatchSampler</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_buckets</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                 <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__pre_init__</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_last</span><span class="p">,</span> <span class="n">shuffle</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span>
                             <span class="n">dynamic</span><span class="p">,</span> <span class="n">sort</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">_item_lengths</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_buckets</span> <span class="o">=</span> <span class="n">num_buckets</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">right_bucket_limits</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span>
            <span class="n">max_length</span><span class="o">/</span><span class="n">num_buckets</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">num_buckets</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">dynamic</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bucket_batch_lengths</span> <span class="o">=</span> <span class="n">batch_size</span><span class="o">//</span><span class="bp">self</span><span class="p">.</span><span class="n">right_bucket_limits</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bucket_batch_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">num_buckets</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">generate_batches</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_generate_batches</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
        <span class="n">batches</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_buckets</span><span class="p">)]</span>
        <span class="n">current_batches</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_buckets</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">:</span>
            <span class="n">item_idx</span><span class="p">,</span> <span class="n">item_length</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_item_lengths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">bucket_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">searchsorted</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">right_bucket_limits</span><span class="p">,</span> <span class="n">item_length</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">bucket_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_buckets</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">item_length</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">max_length</span><span class="p">:</span>
                    <span class="n">bucket_idx</span> <span class="o">-=</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">'found an item that is longer than the '</span>
                                     <span class="s">'maximum item length'</span><span class="p">)</span>
            <span class="n">current_batches</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">].</span><span class="n">append</span><span class="p">((</span><span class="n">item_idx</span><span class="p">,</span> <span class="n">item_length</span><span class="p">))</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">current_batches</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">])</span> \
                    <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">bucket_batch_lengths</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">]:</span>
                <span class="n">batches</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">current_batches</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">])</span>
                <span class="n">current_batches</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">current_batches</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">])</span> \
                    <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">bucket_batch_lengths</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">'maximum number of items in bucket exceeded'</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">.</span><span class="n">drop_last</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">bucket_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">current_batches</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">batches</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">batches</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">batches</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
</code></pre></div></div>

  </div></div>

    </section>
    <footer class="condensed">
      <ul class="social about-footer condensed"><a href="https://github.com/philgzl" target="_blank">
          <li>
            <i class="icon-github-circled"></i>
          </li>
        </a><a href="https://www.linkedin.com/in/philgzl" target="_blank">
          <li>
            <i class="icon-linkedin-squared"></i>
          </li>
        </a><a href="mailto:hello@philgzl.com" target="_blank">
          <li>
            <i class="icon-mail-alt"></i>
          </li>
        </a></ul><nav class="navigation about-footer condensed">
        <ul>
          
          <li>
            <a href="/about">About</a>
          </li>
          
        </ul>
      </nav><p class="about-footer condensed">&copy;
        2023</p><div class="about-footer condensed">
        <p>Dark Mode
          <i class="icon-moon"></i>
          <label class="switch">
            <input type="checkbox" class="dark-mode-toggle">
            <span class="slider round" onclick="toggleDarkMode()"></span>
          </label>
        </p>
      </div>
    </footer>
  </main>
  
  <script type="text/javascript" src="/assets/js/darkmode.js"></script>
  
  <script src="/assets/js/simple-jekyll-search.min.js"></script>
  <script src="/assets/js/search.js"></script>
  
</body>

</html>
